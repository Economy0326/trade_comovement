{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "016497ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pivot] (100, 43) | months: 2022-01 -> 2025-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build_pair_features(clf): 100%|██████████| 100/100 [00:18<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[clf pseudo-label] pos: (1991, 14) | neg: (6565, 14)\n",
      "[clf train set] (5973, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score_all_pairs: 100%|██████████| 100/100 [00:50<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pair 선택] 실제 제출 3500개 | 목표K=3500 | use_tau=True tau=0.28 | use_stab=False stab=OFF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build_reg_dataset: 3500it [00:01, 2821.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[reg train set] (123556, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "make_submission: 3500it [00:07, 455.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 저장 완료: firststyle_k3500_tau028_ble095_backfill.csv | shape=(3500, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# [제출 전용] 공행성(pair) 판별 + 다음달(value) 예측\n",
    "# - validation 없음 (제출 무제한 가정)\n",
    "# - \"처음 0.34 나오던\" 상관 기반 구조를 최대한 유지\n",
    "# - 주석 한국어 상세\n",
    "#\n",
    "# 핵심 개선 포인트(이번 버전):\n",
    "# 1) pair 선택 시 PROB_TAU를 써도 \"항상 K개\" 제출하도록 backfill(빈자리 채움)\n",
    "#    -> tau가 빡세서 K를 못 채우면 FN(놓침) 증가로 점수 급락하는 현상 방지\n",
    "# 2) 회귀 예측은 XGBRegressor로 하고, follower 자체 MA3 baseline 블렌딩 옵션 유지\n",
    "# ============================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 0) ✅ 실험/제출용 하이퍼파라미터 (여기만 바꿔가며 제출)\n",
    "# =========================================================\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# (1) lag(선행-후행 시차) 후보 범위: 1~MAX_LAG\n",
    "MAX_LAG = 7\n",
    "\n",
    "# (2) pseudo-label(약지도) 생성 시 positive로 찍는 상관 임계값\n",
    "#     - 너무 높으면 pos가 너무 적어져서 classifier가 약해짐\n",
    "#     - 너무 낮으면 pos가 너무 많아져서 classifier가 구분을 못함\n",
    "PAIR_LABEL_CORR_THRESHOLD = 0.38  # 0.34~0.40 sweep 추천\n",
    "\n",
    "# (3) classifier 학습용: 너무 0이 많은 품목 제외(희소성 노이즈 줄이기)\n",
    "PAIR_MIN_NONZERO = 8\n",
    "\n",
    "# (4) 전체 pair 스코어링용: min_nonzero를 완화해서 recall 확보\n",
    "SCORE_MIN_NONZERO = 2\n",
    "\n",
    "# (5) negative sampling 비율 (pos:neg = 1:NEG_POS_RATIO)\n",
    "NEG_POS_RATIO = 2.0\n",
    "\n",
    "# (6) ✅ 최종 제출 pair 개수 (가장 중요)\n",
    "PAIR_TOP_K = 3500  # 2500, 3000, 3500, 4000 등 sweep\n",
    "\n",
    "# (7) ✅ classifier 확률 컷 (FP 줄이기 위한 옵션)\n",
    "#     - 이번 버전은 \"tau를 걸어도 K를 반드시 채우도록(backfill)\" 설계했으므로\n",
    "#       tau를 써도 FN 폭증 위험이 훨씬 줄어듦\n",
    "USE_PROB_THRESHOLD = True\n",
    "PROB_TAU = 0.28  # 0.22~0.35 sweep 추천\n",
    "\n",
    "# (8) (선택) corr_stability 필터: 우연 상관 제거\n",
    "#     - 너무 빡세면 FN 늘어서 점수 떨어질 수 있음\n",
    "USE_STABILITY_FILTER = False\n",
    "STAB_MAX = 0.20  # 켜면 0.15~0.30 sweep\n",
    "\n",
    "# (9) 회귀 예측 안정화: follower 자체 baseline(ma3) 블렌딩\n",
    "USE_BASELINE_BLEND = True\n",
    "BLEND_ALPHA = 0.95  # 0.90~0.98 sweep 추천 (너 결과상 0.95가 괜찮은 편)\n",
    "\n",
    "# (10) 회귀 모델 파라미터 (너무 자주 건드릴 필요는 없음)\n",
    "REG_N_ESTIMATORS = 600\n",
    "REG_MAX_DEPTH = 4\n",
    "REG_LR = 0.05\n",
    "\n",
    "# (11) 분류기 파라미터 (처음 스타일 유지)\n",
    "CLF_N_ESTIMATORS = 200\n",
    "CLF_MAX_DEPTH = 4\n",
    "CLF_LR = 0.08\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1) 유틸: 안전 상관계수 계산\n",
    "# =========================================================\n",
    "def safe_corr(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Pearson 상관계수를 안전하게 계산\n",
    "    - NaN 제거\n",
    "    - 샘플 수 너무 적으면 0\n",
    "    - 분산 0이면 0\n",
    "    \"\"\"\n",
    "    mask = (~np.isnan(a)) & (~np.isnan(b))\n",
    "    if mask.sum() < 3:\n",
    "        return 0.0\n",
    "    aa, bb = a[mask], b[mask]\n",
    "    if np.std(aa) == 0 or np.std(bb) == 0:\n",
    "        return 0.0\n",
    "    return float(np.corrcoef(aa, bb)[0, 1])\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2) 데이터 로딩: 월별 value pivot 생성\n",
    "# =========================================================\n",
    "def load_pivot(train_path=\"train.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    train.csv -> item_id x 월(ym) pivot 생성\n",
    "    - 월별 합계(value sum)가 문제 정의에 맞음\n",
    "    - 결측 월은 0으로 채움\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(train_path)\n",
    "\n",
    "    monthly = (\n",
    "        df.groupby([\"item_id\", \"year\", \"month\"], as_index=False)[\"value\"]\n",
    "          .sum()\n",
    "    )\n",
    "\n",
    "    monthly[\"ym\"] = pd.to_datetime(\n",
    "        monthly[\"year\"].astype(str) + \"-\" +\n",
    "        monthly[\"month\"].astype(str).str.zfill(2) + \"-01\"\n",
    "    )\n",
    "\n",
    "    pivot = monthly.pivot(index=\"item_id\", columns=\"ym\", values=\"value\")\n",
    "    pivot = pivot.fillna(0).sort_index(axis=1)\n",
    "\n",
    "    print(\"[pivot]\", pivot.shape,\n",
    "          \"| months:\", pivot.columns[0].strftime(\"%Y-%m\"), \"->\", pivot.columns[-1].strftime(\"%Y-%m\"))\n",
    "    return pivot\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3) pair의 lag-corr 통계 피처 계산 (핵심)\n",
    "# =========================================================\n",
    "def calc_pair_stats(a: np.ndarray, b: np.ndarray, max_lag: int) -> dict:\n",
    "    \"\"\"\n",
    "    a가 선행, b가 후행이라고 가정하고\n",
    "    lag=1..max_lag에 대해 corr(a[t], b[t+lag]) 계산.\n",
    "\n",
    "    반환:\n",
    "    - max_corr: 가장 큰(절대값 기준) 상관\n",
    "    - best_lag: 그때의 lag\n",
    "    - second_corr: 2등 상관\n",
    "    - corr_stability: |best - second| (작을수록 '비슷한 lag들에서 안정적')\n",
    "    - corr_mean/std/abs_mean: lag별 상관 통계\n",
    "    \"\"\"\n",
    "    n = len(a)\n",
    "    lag_corrs = []\n",
    "\n",
    "    best_corr = 0.0\n",
    "    second_corr = 0.0\n",
    "    best_lag = None\n",
    "\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        if n <= lag:\n",
    "            lag_corrs.append(0.0)\n",
    "            continue\n",
    "\n",
    "        c = safe_corr(a[:-lag], b[lag:])\n",
    "        lag_corrs.append(c)\n",
    "\n",
    "        # 절대값 기준으로 1등/2등 갱신\n",
    "        if abs(c) > abs(best_corr):\n",
    "            second_corr = best_corr\n",
    "            best_corr = c\n",
    "            best_lag = lag\n",
    "        elif abs(c) > abs(second_corr):\n",
    "            second_corr = c\n",
    "\n",
    "    if best_lag is None:\n",
    "        best_lag = 1\n",
    "\n",
    "    lag_corrs = np.array(lag_corrs, dtype=float)\n",
    "\n",
    "    return {\n",
    "        \"max_corr\": float(best_corr),\n",
    "        \"best_lag\": int(best_lag),\n",
    "        \"second_corr\": float(second_corr),\n",
    "        \"corr_stability\": float(abs(best_corr - second_corr)),\n",
    "        \"corr_mean\": float(np.mean(lag_corrs)),\n",
    "        \"corr_std\": float(np.std(lag_corrs)),\n",
    "        \"corr_abs_mean\": float(np.mean(np.abs(lag_corrs))),\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4) classifier 학습용 pseudo-label 데이터 생성\n",
    "# =========================================================\n",
    "def build_pair_feature_matrix(\n",
    "    pivot: pd.DataFrame,\n",
    "    max_lag: int,\n",
    "    min_nonzero: int,\n",
    "    corr_threshold_for_label: float,\n",
    "    neg_pos_ratio: float\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    정답 pair가 없으니, 상관이 큰 pair를 positive로 가정해 pseudo-label 생성.\n",
    "    - label=1: abs(max_corr) >= corr_threshold_for_label\n",
    "    - label=0: 그 외\n",
    "    \"\"\"\n",
    "    items = pivot.index.to_list()\n",
    "\n",
    "    rows_pos, rows_neg = [], []\n",
    "\n",
    "    for leader in tqdm(items, desc=\"build_pair_features(clf)\"):\n",
    "        a = pivot.loc[leader].values.astype(float)\n",
    "        if np.count_nonzero(a) < min_nonzero:\n",
    "            continue\n",
    "\n",
    "        for follower in items:\n",
    "            if follower == leader:\n",
    "                continue\n",
    "\n",
    "            b = pivot.loc[follower].values.astype(float)\n",
    "            if np.count_nonzero(b) < min_nonzero:\n",
    "                continue\n",
    "\n",
    "            feats = calc_pair_stats(a, b, max_lag)\n",
    "            label = 1 if abs(feats[\"max_corr\"]) >= corr_threshold_for_label else 0\n",
    "\n",
    "            row = {\n",
    "                \"leading_item_id\": leader,\n",
    "                \"following_item_id\": follower,\n",
    "                **feats,\n",
    "                \"nonzero_a\": int(np.count_nonzero(a)),\n",
    "                \"nonzero_b\": int(np.count_nonzero(b)),\n",
    "                \"sum_a\": float(a.sum()),\n",
    "                \"sum_b\": float(b.sum()),\n",
    "                \"label\": int(label),\n",
    "            }\n",
    "\n",
    "            if label == 1:\n",
    "                rows_pos.append(row)\n",
    "            else:\n",
    "                rows_neg.append(row)\n",
    "\n",
    "    df_pos = pd.DataFrame(rows_pos)\n",
    "    df_neg = pd.DataFrame(rows_neg)\n",
    "    print(\"[clf pseudo-label] pos:\", df_pos.shape, \"| neg:\", df_neg.shape)\n",
    "\n",
    "    if df_pos.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # negative sampling (학습 속도/불균형 조절)\n",
    "    n_pos = len(df_pos)\n",
    "    n_neg_keep = int(neg_pos_ratio * n_pos)\n",
    "    if len(df_neg) > n_neg_keep:\n",
    "        df_neg = df_neg.sample(n_neg_keep, random_state=RANDOM_SEED)\n",
    "\n",
    "    df_all = pd.concat([df_pos, df_neg], axis=0).reset_index(drop=True)\n",
    "    print(\"[clf train set]\", df_all.shape)\n",
    "    return df_all\n",
    "\n",
    "\n",
    "def train_pair_classifier(df_pairs: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    XGBClassifier로 pair가 공행성일 확률(clf_prob)을 예측하도록 학습.\n",
    "    \"\"\"\n",
    "    feature_cols = [\n",
    "        \"max_corr\", \"best_lag\", \"second_corr\",\n",
    "        \"corr_stability\", \"corr_mean\", \"corr_std\", \"corr_abs_mean\",\n",
    "        \"nonzero_a\", \"nonzero_b\", \"sum_a\", \"sum_b\"\n",
    "    ]\n",
    "\n",
    "    df = df_pairs.copy()\n",
    "    df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "    X = df[feature_cols].values\n",
    "    y = df[\"label\"].values\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        n_estimators=CLF_N_ESTIMATORS,\n",
    "        max_depth=CLF_MAX_DEPTH,\n",
    "        learning_rate=CLF_LR,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=1.0,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.2,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1,\n",
    "        eval_metric=\"logloss\"\n",
    "    )\n",
    "    clf.fit(X, y)\n",
    "    return clf, feature_cols\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5) 전체 pair scoring: clf_prob 계산 + stats 저장\n",
    "# =========================================================\n",
    "def score_all_pairs(pivot: pd.DataFrame, clf, clf_feature_cols, max_lag: int, min_nonzero: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    모든 (leader -> follower) 쌍에 대해:\n",
    "    - lag-corr 통계 피처 계산\n",
    "    - classifier 확률 clf_prob 계산\n",
    "    \"\"\"\n",
    "    items = pivot.index.to_list()\n",
    "    rows = []\n",
    "\n",
    "    for leader in tqdm(items, desc=\"score_all_pairs\"):\n",
    "        a = pivot.loc[leader].values.astype(float)\n",
    "        if np.count_nonzero(a) < min_nonzero:\n",
    "            continue\n",
    "\n",
    "        for follower in items:\n",
    "            if follower == leader:\n",
    "                continue\n",
    "\n",
    "            b = pivot.loc[follower].values.astype(float)\n",
    "            if np.count_nonzero(b) < min_nonzero:\n",
    "                continue\n",
    "\n",
    "            feats = calc_pair_stats(a, b, max_lag)\n",
    "            extra = {\n",
    "                \"nonzero_a\": int(np.count_nonzero(a)),\n",
    "                \"nonzero_b\": int(np.count_nonzero(b)),\n",
    "                \"sum_a\": float(a.sum()),\n",
    "                \"sum_b\": float(b.sum()),\n",
    "            }\n",
    "\n",
    "            x_vec = np.array([[{**feats, **extra}[c] for c in clf_feature_cols]], dtype=float)\n",
    "            prob = float(clf.predict_proba(x_vec)[0, 1])\n",
    "\n",
    "            rows.append({\n",
    "                \"leading_item_id\": leader,\n",
    "                \"following_item_id\": follower,\n",
    "                \"clf_prob\": prob,\n",
    "                **feats\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.sort_values(\"clf_prob\", ascending=False).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6) 회귀 데이터셋 생성/학습\n",
    "# =========================================================\n",
    "REG_FEATURE_COLS = [\n",
    "    \"b_t\", \"b_t_1\", \"b_t_2\",\n",
    "    \"b_ma3\", \"b_change\",\n",
    "    \"a_t_lag\", \"a_t_lag_1\",\n",
    "    \"a_ma3\", \"a_change\",\n",
    "    \"ab_value_ratio\",\n",
    "    \"max_corr\", \"best_lag\", \"corr_stability\",\n",
    "]\n",
    "\n",
    "def build_reg_dataset(pivot: pd.DataFrame, pairs: pd.DataFrame, target_end_idx: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    각 pair(A->B)에 대해 여러 시점 t를 만들어 (t+1)을 예측하도록 학습 row 생성.\n",
    "\n",
    "    target_end_idx:\n",
    "    - 학습 타깃의 마지막 인덱스(포함)\n",
    "    - 마지막 관측 월이 last_idx면, (t+1)이 last_idx까지 학습 가능하므로\n",
    "      target_end_idx는 보통 last_idx-1로 둠(아래 run_submission 참고)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for r in tqdm(pairs.itertuples(index=False), desc=\"build_reg_dataset\"):\n",
    "        leader = r.leading_item_id\n",
    "        follower = r.following_item_id\n",
    "        lag = int(r.best_lag)\n",
    "\n",
    "        a = pivot.loc[leader].values.astype(float)\n",
    "        b = pivot.loc[follower].values.astype(float)\n",
    "\n",
    "        # t+1이 타깃이므로 t는 target_end_idx-1까지 가능\n",
    "        # 또한 b_t_2(=t-2), a_t_lag_1(=t-lag-1) 등이 필요하므로 최소 인덱스가 필요\n",
    "        for t in range(lag + 2, target_end_idx):\n",
    "            target_idx = t + 1\n",
    "            if target_idx > target_end_idx:\n",
    "                continue\n",
    "\n",
    "            # follower 최근 값들\n",
    "            b_t = b[t]\n",
    "            b_t_1 = b[t - 1]\n",
    "            b_t_2 = b[t - 2]\n",
    "            b_ma3 = (b_t + b_t_1 + b_t_2) / 3.0\n",
    "            b_change = (b_t - b_t_1) / (b_t_1 + 1.0)\n",
    "\n",
    "            # leader lag 적용 값들\n",
    "            a_t_lag = a[t - lag]\n",
    "            a_t_lag_1 = a[t - lag - 1]\n",
    "            a_change = (a_t_lag - a_t_lag_1) / (a_t_lag_1 + 1.0)\n",
    "\n",
    "            # leader도 3개월 평균(가능하면)\n",
    "            if (t - lag - 2) >= 0:\n",
    "                a_ma3 = (a_t_lag + a_t_lag_1 + a[t - lag - 2]) / 3.0\n",
    "            else:\n",
    "                a_ma3 = (a_t_lag + a_t_lag_1) / 2.0\n",
    "\n",
    "            # ratio\n",
    "            ab_ratio = b_t / (a_t_lag + 1.0)\n",
    "\n",
    "            y = b[target_idx]\n",
    "\n",
    "            rows.append({\n",
    "                \"leading_item_id\": leader,\n",
    "                \"following_item_id\": follower,\n",
    "\n",
    "                \"b_t\": b_t,\n",
    "                \"b_t_1\": b_t_1,\n",
    "                \"b_t_2\": b_t_2,\n",
    "                \"b_ma3\": b_ma3,\n",
    "                \"b_change\": b_change,\n",
    "\n",
    "                \"a_t_lag\": a_t_lag,\n",
    "                \"a_t_lag_1\": a_t_lag_1,\n",
    "                \"a_ma3\": a_ma3,\n",
    "                \"a_change\": a_change,\n",
    "\n",
    "                \"ab_value_ratio\": ab_ratio,\n",
    "\n",
    "                \"max_corr\": float(r.max_corr),\n",
    "                \"best_lag\": lag,\n",
    "                \"corr_stability\": float(r.corr_stability),\n",
    "\n",
    "                \"target\": y\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(\"[reg train set]\", df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_regressor(df_train: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    XGBRegressor 학습\n",
    "    \"\"\"\n",
    "    df = df_train.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "    X = df[REG_FEATURE_COLS].values\n",
    "    y = df[\"target\"].values\n",
    "\n",
    "    reg = XGBRegressor(\n",
    "        n_estimators=REG_N_ESTIMATORS,\n",
    "        max_depth=REG_MAX_DEPTH,\n",
    "        learning_rate=REG_LR,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.85,\n",
    "        min_child_weight=5,\n",
    "        gamma=0.2,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    reg.fit(X, y)\n",
    "    return reg\n",
    "\n",
    "\n",
    "def follower_ma3(b: np.ndarray, t: int) -> float:\n",
    "    \"\"\"\n",
    "    follower 자체 baseline: 최근 3개월 평균\n",
    "    \"\"\"\n",
    "    if t < 2:\n",
    "        return float(b[t])\n",
    "    return float((b[t] + b[t-1] + b[t-2]) / 3.0)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7) ✅ 제출 파일 생성 (submission-only)\n",
    "# =========================================================\n",
    "def run_submission(train_path=\"train.csv\", out_path=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    전체 파이프라인(제출 전용)\n",
    "\n",
    "    1) pivot 생성\n",
    "    2) pseudo-label 생성 -> pair classifier 학습\n",
    "    3) 모든 pair clf_prob 스코어링\n",
    "    4) pair 선택: (옵션) stability 필터 + (옵션) tau 필터\n",
    "       - 핵심: tau로 거른 뒤에도 \"항상 K개\" 채우도록 backfill\n",
    "    5) 회귀 학습\n",
    "    6) 마지막 월 기준 다음달(value) 예측 -> CSV 저장\n",
    "    \"\"\"\n",
    "    pivot = load_pivot(train_path)\n",
    "    months = list(pivot.columns)\n",
    "\n",
    "    last_idx = len(months) - 1     # 마지막 관측 월 인덱스 (예: 2025-07)\n",
    "    target_end_idx = last_idx - 1  # 회귀 학습에서 target은 t+1 이므로 마지막 월은 target으로 못 씀\n",
    "\n",
    "    # ---- (A) pseudo-label 생성 -> classifier 학습\n",
    "    df_pairs_label = build_pair_feature_matrix(\n",
    "        pivot=pivot,\n",
    "        max_lag=MAX_LAG,\n",
    "        min_nonzero=PAIR_MIN_NONZERO,\n",
    "        corr_threshold_for_label=PAIR_LABEL_CORR_THRESHOLD,\n",
    "        neg_pos_ratio=NEG_POS_RATIO\n",
    "    )\n",
    "    if df_pairs_label.empty:\n",
    "        raise RuntimeError(\"pseudo-label pos가 0개입니다. PAIR_LABEL_CORR_THRESHOLD를 낮춰보세요.\")\n",
    "\n",
    "    clf, clf_feature_cols = train_pair_classifier(df_pairs_label)\n",
    "\n",
    "    # ---- (B) 전체 pair 스코어링\n",
    "    pairs_all = score_all_pairs(\n",
    "        pivot=pivot,\n",
    "        clf=clf,\n",
    "        clf_feature_cols=clf_feature_cols,\n",
    "        max_lag=MAX_LAG,\n",
    "        min_nonzero=SCORE_MIN_NONZERO\n",
    "    )\n",
    "\n",
    "    # ---- (C) 제출 pair 선택\n",
    "    # 0) 기본은 전체에서 prob 높은 순\n",
    "    pairs_base = pairs_all.copy()\n",
    "\n",
    "    # 1) (선택) stability 필터\n",
    "    if USE_STABILITY_FILTER:\n",
    "        pairs_base = pairs_base[pairs_base[\"corr_stability\"] <= STAB_MAX]\n",
    "\n",
    "    # 2) (선택) tau 필터를 먼저 적용하되, K를 못 채우면 backfill\n",
    "    if USE_PROB_THRESHOLD:\n",
    "        pairs_tau = pairs_base[pairs_base[\"clf_prob\"] >= PROB_TAU].copy()\n",
    "\n",
    "        if len(pairs_tau) >= PAIR_TOP_K:\n",
    "            # tau 통과분이 충분하면 그 안에서 topK\n",
    "            pairs_sel = pairs_tau.sort_values(\"clf_prob\", ascending=False).head(PAIR_TOP_K)\n",
    "        else:\n",
    "            # tau 통과분이 부족하면, 남은 자리는 전체(prob 높은 순)에서 추가로 채움\n",
    "            need = PAIR_TOP_K - len(pairs_tau)\n",
    "            # pairs_base 기준으로 채우는 게 일관성 있음(=stability 필터 반영됨)\n",
    "            rest = pairs_base.loc[~pairs_base.index.isin(pairs_tau.index)] \\\n",
    "                             .sort_values(\"clf_prob\", ascending=False) \\\n",
    "                             .head(need)\n",
    "            pairs_sel = pd.concat([pairs_tau, rest], axis=0)\n",
    "    else:\n",
    "        # tau를 안 쓰면 그냥 prob 상위 K\n",
    "        pairs_sel = pairs_base.sort_values(\"clf_prob\", ascending=False).head(PAIR_TOP_K)\n",
    "\n",
    "    pairs_sel = pairs_sel.reset_index(drop=True)\n",
    "\n",
    "    print(\n",
    "        f\"[pair 선택] 실제 제출 {len(pairs_sel)}개 | 목표K={PAIR_TOP_K} | \"\n",
    "        f\"use_tau={USE_PROB_THRESHOLD} tau={PROB_TAU} | \"\n",
    "        f\"use_stab={USE_STABILITY_FILTER} stab={STAB_MAX if USE_STABILITY_FILTER else 'OFF'}\"\n",
    "    )\n",
    "\n",
    "    if len(pairs_sel) == 0:\n",
    "        raise RuntimeError(\"선택된 pair가 0개입니다. PROB_TAU를 낮추거나 필터를 끄세요.\")\n",
    "\n",
    "    # ---- (D) 회귀 학습\n",
    "    df_reg = build_reg_dataset(pivot, pairs_sel, target_end_idx=target_end_idx)\n",
    "    if len(df_reg) == 0:\n",
    "        raise RuntimeError(\"회귀 학습 데이터가 0개입니다. PAIR_TOP_K를 늘리거나 필터를 완화하세요.\")\n",
    "\n",
    "    reg = train_regressor(df_reg)\n",
    "\n",
    "    # ---- (E) 마지막 관측 월(last_idx)을 기준으로 다음달 예측\n",
    "    sub_rows = []\n",
    "\n",
    "    for r in tqdm(pairs_sel.itertuples(index=False), desc=\"make_submission\"):\n",
    "        leader = r.leading_item_id\n",
    "        follower = r.following_item_id\n",
    "        lag = int(r.best_lag)\n",
    "\n",
    "        a = pivot.loc[leader].values.astype(float)\n",
    "        b = pivot.loc[follower].values.astype(float)\n",
    "\n",
    "        t = last_idx  # 마지막 관측 월을 t로 보고 (t+1)을 예측\n",
    "\n",
    "        # 필요한 과거값 체크\n",
    "        if t - 2 < 0:\n",
    "            continue\n",
    "        if t - lag - 1 < 0:\n",
    "            continue\n",
    "\n",
    "        # follower\n",
    "        b_t = b[t]\n",
    "        b_t_1 = b[t - 1]\n",
    "        b_t_2 = b[t - 2]\n",
    "        b_ma3 = (b_t + b_t_1 + b_t_2) / 3.0\n",
    "        b_change = (b_t - b_t_1) / (b_t_1 + 1.0)\n",
    "\n",
    "        # leader (lag 적용)\n",
    "        a_t_lag = a[t - lag]\n",
    "        a_t_lag_1 = a[t - lag - 1]\n",
    "        a_change = (a_t_lag - a_t_lag_1) / (a_t_lag_1 + 1.0)\n",
    "\n",
    "        if (t - lag - 2) >= 0:\n",
    "            a_ma3 = (a_t_lag + a_t_lag_1 + a[t - lag - 2]) / 3.0\n",
    "        else:\n",
    "            a_ma3 = (a_t_lag + a_t_lag_1) / 2.0\n",
    "\n",
    "        ab_ratio = b_t / (a_t_lag + 1.0)\n",
    "\n",
    "        feat_row = {\n",
    "            \"b_t\": b_t,\n",
    "            \"b_t_1\": b_t_1,\n",
    "            \"b_t_2\": b_t_2,\n",
    "            \"b_ma3\": b_ma3,\n",
    "            \"b_change\": b_change,\n",
    "            \"a_t_lag\": a_t_lag,\n",
    "            \"a_t_lag_1\": a_t_lag_1,\n",
    "            \"a_ma3\": a_ma3,\n",
    "            \"a_change\": a_change,\n",
    "            \"ab_value_ratio\": ab_ratio,\n",
    "            \"max_corr\": float(r.max_corr),\n",
    "            \"best_lag\": int(r.best_lag),\n",
    "            \"corr_stability\": float(r.corr_stability),\n",
    "        }\n",
    "\n",
    "        x_row = np.array([[feat_row[c] for c in REG_FEATURE_COLS]], dtype=float)\n",
    "        pred = float(reg.predict(x_row)[0])\n",
    "        pred = max(pred, 0.0)\n",
    "\n",
    "        # (선택) follower baseline과 블렌딩\n",
    "        if USE_BASELINE_BLEND:\n",
    "            base = follower_ma3(b, t)\n",
    "            pred = BLEND_ALPHA * pred + (1.0 - BLEND_ALPHA) * base\n",
    "            pred = max(pred, 0.0)\n",
    "\n",
    "        sub_rows.append({\n",
    "            \"leading_item_id\": leader,\n",
    "            \"following_item_id\": follower,\n",
    "            \"value\": int(round(pred))\n",
    "        })\n",
    "\n",
    "    df_sub = pd.DataFrame(sub_rows).drop_duplicates([\"leading_item_id\", \"following_item_id\"])\n",
    "    df_sub.to_csv(out_path, index=False)\n",
    "    print(f\"\\n✅ 저장 완료: {out_path} | shape={df_sub.shape}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8) 실행부: OUT 파일명만 바꿔서 제출 반복\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # ✅ 파일명만 바꿔가며 제출\n",
    "    OUT = \"firststyle_k3500_tau028_ble095_backfill.csv\"\n",
    "    run_submission(\"train.csv\", OUT)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
